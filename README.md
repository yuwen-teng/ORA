# Reinforcement Learning(RL)
RL is known as a semi-supervised learning model in machine learning, 
is a technique to learn by interacting with its environment. The agent receives rewards by performing correctly and penalties for performing incorrectly. 
The agent learns without intervention from a human by maximizing its reward and minimizing its penalty. 
RL is usually modeled as a Markov Decision Process (MDP).
![Github](https://miro.medium.com/max/1400/1*-0G8EIeG24OYTbt5KZSalQ.png)
>source: [Reinforcement Learning:An Introduction](http://incompleteideas.net/book/bookdraft2017nov5.pdf)


RL is used in different applications:resources management in computer clusters, traffic light control, web system configuration, games.
The most famous one must be AlphaGo and AlphaGo Zero.
# Scheduling with Reinforcement Learning
## Introduction
In recent years, some people have begun to use reinforcement learning for factory and semiconductor plant scheduling.

Take [this paper](https://www.semanticscholar.org/paper/A-REINFORCEMENT-LEARNING-APPROACH-TO-SCHEDULING-Roh-Lee/b61d28e723b9b29b876813226a21d55088c4cdef) 
as an example, with the development in the semiconductor manufacturing industry, quality issues have been considered in the wafer manufacturing process 
In order to avoid quality problems caused by batch production, the cluster tool process one wafer at a time is be used in the semiconductor industry.

The cluster tool consists of serval chambers and one transport robot and only one wafer is processed at a time in each chamber.
The transport robot changes the wafers inside the cluster tool.
When the wafer is completed in the chamber,the wafer is moved to the next chamber by a robot. 
The transport robot repeats the process of loading the wafer to appropriate chamber and unloading the completed wafer from the chamber.
Because the cluster tool only has a transport robot, the wafers can't be changed at any time. If serval chambers had already finished the wafer,the cluster tools
can't move all complete wafers at the same time.
## Methdology
Roh and Lee developed a model using Markov Decision Process (MDP), and the model can handle time variants in the cluster tools.

First, the state and action will be defined. The state is set to (ğ‘Š, ğ¶1, ğ¶2, â€¦ , ğ¶ğ‘› , ğ‘…, ğ‘†1, ğ‘†2, ğ‘1, ğ‘2, â€¦ , ğ‘ğ‘› ),where W is the number of wafers remaining in the loadlock.
ğ¶ğ‘– is the state of the chamber, which represents whether the ğ‘– th chamber is empty ( ğ¶ğ‘– = 0), full ( ğ¶ğ‘– = 1), or completed processing (ğ¶ğ‘– = 2) for ğ‘– âˆˆ {1, 2, â€¦ , ğ‘›}.
ğ‘… is the number of wafers held by the transport robot. ğ‘†1 ğ‘ğ‘›ğ‘‘ ğ‘†2 are the next process steps of the wafers held by the robot. ğ‘ğ‘– is the expected remaining process time of the
ğ‘–th chamber for ğ‘– âˆˆ {1, 2, â€¦ , ğ‘›}. The action is set to {ğ‘Šğ‘ğ‘–ğ‘¡, ğ‘ˆğ‘— , ğ¿ğ‘— , ğ‘†ğ‘Šğ‘– } , where ğ‘— âˆˆ {0 ,1, 2, â€¦ , ğ‘›} ğ‘ğ‘›ğ‘‘ ğ‘– âˆˆ {1, 2, â€¦ , ğ‘›}. They all represent the robot tasks: ğ‘ˆğ‘—, ğ¿ğ‘—, and ğ‘†ğ‘Šğ‘– indicate unload, load, and swap operations on the ğ‘— th or ğ‘– th chamber. Accroding to figure 1, the state is (ğ‘Š = 3, ğ¶1 = 2, ğ¶2 = 1, ğ¶3 =
1, ğ¶4 = 2, ğ‘… = 1, ğ‘†1 = 2, ğ‘†2 = 0, ğ‘1 = 0, ğ‘2 = 4, ğ‘3 = 4, ğ‘4 = 0).Suppose that the black and white wafers indicate that they are in the first and second process steps, respectively, and the hatched wafer indicates that the process is in progress (Roh and Lee,2017).
[figure 1: A Dual-armed Cluster Tool with Four Chambers](https://github.com/yuwen-teng/ORA/blob/master/A%20Dual-armed%20Cluster%20Tool%20with%20Four%20Chambers.PNG)

Second, the invalid action and deadlock action will be eliminate. To eliminate the invalid action for the current state, the agent requests theenvironment for the results for all the actions, then holds valid actions. After agent takes the valid action, cause the next state does't has the valid state, is called deadlock action.

Third, the agent will take the Q-learning algorithm to find the best action for each state.

